{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinManojAlex/NLP_Tamil_Hindi/blob/main/machine_translation_model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the GDrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3iJEHnlZOM7",
        "outputId": "c9205fa2-dc1a-4f88-c6f7-001e4dd6df55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Reading datasets collected"
      ],
      "metadata": {
        "id": "hxLuzo7xCMDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Reading the Hindi-English Parallel Corpus"
      ],
      "metadata": {
        "id": "mM5Y423i30Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_en_hi = []\n",
        "train_hi_hi = []\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_hi/cvit_train.en.txt', 'r') as file:\n",
        "  for line in file:\n",
        "\n",
        "    # Appending the stripped english sentences into the list\n",
        "    train_en_hi.append(line.strip())\n",
        "    \n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_hi/cvit_train.hi.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    \n",
        "    # Appending the stripped hindi sentences into the list\n",
        "    train_hi_hi.append(line.strip())\n",
        "\n",
        "\n",
        "print(f'{len(train_en_hi)} english lines read from PIB_English_Hindi Corpus')\n",
        "print(f'{len(train_hi_hi)} hindi lines read from PIB_English_Hindi Corpus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbCHbDF_gsNe",
        "outputId": "276a74c4-20ac-401d-e80e-6574c108326e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "266545 english lines read from PIB_English_Hindi Corpus\n",
            "266545 hindi lines read from PIB_English_Hindi Corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Reading the Tamil-English Parallel corpus"
      ],
      "metadata": {
        "id": "9TgR1W3x3rSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_en_ta = []\n",
        "train_ta_ta = []\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_ta/cvit_train.en.txt', 'r') as file:\n",
        "  for line in file:\n",
        "\n",
        "    # Appending the stripped english sentences into the list\n",
        "    train_en_ta.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_ta/cvit_train.ta.txt', 'r') as file:\n",
        "  for line in file:\n",
        "\n",
        "    # Appending the stripped tamil sentences into the list\n",
        "    train_ta_ta.append(line.strip())\n",
        "\n",
        "print(f'{len(train_en_ta)} english lines read from PIB_English_Tamil Corpus')\n",
        "print(f'{len(train_ta_ta)} tamil lines read from PIB_English_Tamil Corpus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATHKIYyHgWEN",
        "outputId": "2a8b2765-4bf5-4611-f728-0a0a5c1ada70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115968 english lines read from PIB_English_Tamil Corpus\n",
            "115968 tamil lines read from PIB_English_Tamil Corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_hi_hi[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w62oBOsWLKXz",
        "outputId": "2a08b356-9980-40ff-9b96-b803da261fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "प्रधानमंत्री कार्यालय इंडिया टुडे कॉनक्लेव में प्रधान मंत्री के भाषण (वीडियो कान्फ्रसिंग के माध्यम से) का मूल पाठ सबसे पहले आप सभी को इस आयोजन के लिए बहुत-बहुत बधाई-शुभकामनाएं।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_ta_ta[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYZBHBsSLhUI",
        "outputId": "910ab5cf-a1af-422d-9b83-8387cd190117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "முறையை அமல்படுத்துவதற்கு வசதியாக, பல்வேறு சரக்கு மற்றும் சேவைகளுக்கான மேல் வரி மற்றும் கூடுதல் வரியை நீக்கும் வகையில், சுங்கம் மற்றும் கலால் சட்டத்தில் திருத்தங்களைக் கொண்டுவர மத்திய அமைச்சரவை ஒப்புதல் பிரதமர் திரு.நரேந்திர மோடி தலைமையில் மத்திய அமைச்சரவைக் கூட்டம் நடைபெற்றது. இதில், கீழ்க்காணும் பரிந்துரைகளுக்கு அப்போது ஒப்புதல் அளிக்கப்பட்டது.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Correcting the lists so that it matches"
      ],
      "metadata": {
        "id": "aXT057cw0UHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamil-English corpus contains fewer sentences than the Hindi-English corpus. And to do the translation between Hindi and Tamil, we will be required to modify the corpus such that Hindi and Tamil sentences match up."
      ],
      "metadata": {
        "id": "6Qi4GZHQ09Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_hi = []\n",
        "# train_ta = []\n",
        "\n",
        "# for i in range(0, len(train_en_ta)):\n",
        "#   for j in range(0, len(train_en_hi)):\n",
        "    \n",
        "#     if train_en_ta[i] == train_en_hi[j]:\n",
        "#       train_ta.append(train_ta_ta[i])\n",
        "#       train_hi.append(train_hi_hi[j])\n",
        "#       break\n",
        "    \n",
        "#     else:\n",
        "#       continue\n",
        "\n",
        "# print(len(train_hi))\n",
        "# print(len(train_ta))"
      ],
      "metadata": {
        "id": "hzKFtJyN0aSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving this to a file as a new parallel corpus"
      ],
      "metadata": {
        "id": "7eoGArQ8VwzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.hi.txt', 'w+') as file:\n",
        "#   for x in train_hi:\n",
        "#     file.write(x+'\\n')\n",
        "\n",
        "# with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.ta.txt', 'w+') as file:\n",
        "#   for x in train_ta:\n",
        "#     file.write(x+'\\n')"
      ],
      "metadata": {
        "id": "UGg1jfqqVwRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_hi = []\n",
        "train_ta = []\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.hi.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    train_hi.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.ta.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    train_ta.append(line.strip())"
      ],
      "metadata": {
        "id": "lG9NHfnekGxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_hi[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liviu8gdrO0A",
        "outputId": "98d7f9be-6cfb-4c53-b685-60e6b6ae8e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "मंत्रिमण्डल मंत्रिमंडल ने सीमा एवं उत्पाद शुल्क अधिनियम में सेसों के उन्मूलन और विभिन्न वस्तुओं और सेवाओं पर अधिभार के संबंध में जीएसटी व्यवस्था को लागू करने के लिए संशोधन को अनुमोदित किया प्रधानमंत्री श्री नरेंद्र मोदी की अध्यक्षता में केंद्रीय मंत्रिमंडल ने निम्नलिखित प्रस्तावों को मंजूरी प्रदान कर दी।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data preprocessing"
      ],
      "metadata": {
        "id": "Lq7Ff8nwhVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing `tensorflow-text` and `einops` (Einstein Inspired Notation)"
      ],
      "metadata": {
        "id": "iRlRJt8TheGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow package for text related operations and modules\n",
        "!pip install tensorflow-text\n",
        "\n",
        "# Installing einops for writing deep learning code better and more efficiently\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "1d6iiKhhhdat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6029fe-ad9c-4dd2-9bd1-9db4a72efad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (0.13.0)\n",
            "Collecting tensorflow<2.13,>=2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
            "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.31.0)\n",
            "Collecting tensorboard<2.13,>=2.12\n",
            "  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.3.0)\n",
            "Collecting keras<2.13,>=2.12.0\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (67.6.0)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 KB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.4.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
            "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 KB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.51.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (15.0.6.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (4.5.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.10.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.16.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (6.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.2.2)\n",
            "Installing collected packages: wrapt, tensorflow-estimator, tensorboard-data-server, protobuf, keras, tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.11.0\n",
            "    Uninstalling keras-2.11.0:\n",
            "      Successfully uninstalled keras-2.11.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.11.2\n",
            "    Uninstalling tensorboard-2.11.2:\n",
            "      Successfully uninstalled tensorboard-2.11.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.11.0\n",
            "    Uninstalling tensorflow-2.11.0:\n",
            "      Successfully uninstalled tensorflow-2.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.22.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.12.0 protobuf-4.22.1 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-text-2.12.0 wrapt-1.14.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the packages"
      ],
      "metadata": {
        "id": "TzqlrJ9Nifxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "import time\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "DtQEe-yiihmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every sentence should be treated as a `tf.string`, since we are trying to export this model as `tf.saved_model`."
      ],
      "metadata": {
        "id": "BMINoCNFkLuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Normalizing the sentences\n",
        "\n",
        "P.S. Initially training the model without removing the bracketed words from the hindi corpus"
      ],
      "metadata": {
        "id": "wK1VP-0dcgiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Unicode Normalization\n",
        "\n",
        "UNICODE Normalization is essential to maintain accuracy and efficiency in language translation models. For both Hindi and Tamil languages we will be using `NFD` (Normalization Form - D), which is a 'Canonical Decomposition followed by Canonical Composition'. This ensures that equivalent characters are represented in a consistent way.\n",
        "\n",
        "<b>Reference:</b>\n",
        "\n",
        "https://unicode.org/reports/tr15/"
      ],
      "metadata": {
        "id": "t4bKO6jVfWIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that takes in a tensor and normalizes it according to NFC and returns the text\n",
        "\n",
        "def unicode_to_ascii(text):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# temp = tf.constant(train_hi_hi[1])\n",
        "# unicode_normalize(temp)\n",
        "# print(temp)"
      ],
      "metadata": {
        "id": "FZRxrsPulWsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Converting sentence to lowercase\n",
        "\n",
        "This is done to eliminate ambiguity, since we are using English language as an intermediary so it is neccessary to convert the english corpus to its lowercase form so that the model will see 'Car' and 'car' as the same word."
      ],
      "metadata": {
        "id": "SOkWLCYkfvx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 Replacing some special characters\n",
        "\n",
        "Characters like `?`, `!`, `.`, `,` and ` ` &nbsp;must not be removed from the sentence, while the other special characters must be removed. This is done by using regex to filter out the unwanted characters. Hence, we have to make a regex that does not filter out the Hindi and Tamil characters.\n",
        "\n",
        "<b>References:</b>\n",
        "\n",
        "https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)#:~:text=Devanagari%20is%20a%20Unicode%20block,from%20the%201988%20ISCII%20standard\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tamil_(Unicode_block)\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, followed by keeping a white space between the punctuations."
      ],
      "metadata": {
        "id": "zUFxm6lTgJA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Hindi and Tamil characters using UNICODE and then including that in the regex\n",
        "\n",
        "# UNICODE for Hindi characters are stored continously, so we will use a loop to make our list of Hindi characters\n",
        "\n",
        "hindi_characters = 128\n",
        "hindi_unicode_shift = 0x0900\n",
        "\n",
        "hindi_alphabets = []\n",
        "\n",
        "for i in range(0, hindi_characters):\n",
        "  hindi_alphabets.append('\\\\u0'+hex(hindi_unicode_shift+i)[2:])\n",
        "\n",
        "# UNICODE for Tamil characters are not stored continuously since they have some reserved UNICODE characters in between, so we will manually add them to our list\n",
        "\n",
        "tamil_alphabets = ['\\\\u0b82', '\\\\u0b83', '\\\\u0b85', '\\\\u0b86', '\\\\u0b87', '\\\\u0b88', '\\\\u0b89', '\\\\u0b8a', '\\\\u0b8e', '\\\\u0b8f', '\\\\u0b90', '\\\\u0b92', '\\\\u0b93', \n",
        "                   '\\\\u0b94', '\\\\u0b95', '\\\\u0b99', '\\\\u09b9a', '\\\\u0b9c', '\\\\u0b9e', '\\\\u0b9f', '\\\\u0ba3', '\\\\u0ba4', '\\\\u0ba8', '\\\\u0ba9', '\\\\u0baa', '\\\\u0bae'\n",
        "                   '\\\\u0baf', '\\\\u0bb0', '\\\\u0bb1', '\\\\u0bb2', '\\\\u0bb3', '\\\\u0bb4', '\\\\u0bb5', '\\\\u0bb6', '\\\\u0bb7', '\\\\u0bb8', '\\\\u0bb9', '\\\\u0bbe', '\\\\u0bbf',\n",
        "                   '\\\\u0bc0', '\\\\u0bc1', '\\\\u0bc2', '\\\\u0bc6', '\\\\u0bc7', '\\\\u0bc8', '\\\\u0bca', '\\\\u0bcb', '\\\\u0bcc', '\\\\u0bcd', '\\\\u0bd0', '\\\\u0bd7', '\\\\u0be6',\n",
        "                   '\\\\u0be7', '\\\\u0be8', '\\\\u0be9', '\\\\u0bea', '\\\\u0beb', '\\\\u0bec', '\\\\u0bed', '\\\\u0bee', '\\\\u0bef', '\\\\u0bf0', '\\\\u0bf1', '\\\\u0bf2', '\\\\u0bf3',\n",
        "                   '\\\\u0bf4', '\\\\u0bf5', '\\\\u0bf6', '\\\\u0bf7', '\\\\u0bf8', '\\\\u0bf9', '\\\\u0bfa']"
      ],
      "metadata": {
        "id": "Dldo4bB4t4sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that takes in a tensor and keeps `?`, `!`, `.`, `,`, ` ` as such and replaces the other special characters with ``\n",
        "# After that a white space is kept between the 'chosen' punctuations\n",
        "# Account for regex with Hindi and Tamil\n",
        "# Also account for more than 1 whitespace being generated\n",
        "\n",
        "import re\n",
        "\n",
        "def punctuate(text):\n",
        "  text = unicode_to_ascii(text)\n",
        "  regex_pattern = r\"[^,.?! A-Za-z\\u0900-\\u097F\\u0b82\\u0b83\\u0b85\\u0b86\\u0b87\\u0b88\\u0b89\\u0b8a\\u0b8e\\u0b8f\\u0b90\\u0b92\\u0b93\\u0b94\\u0b95\\u0b99\\u09b9a\\u0b9c\\u0b9e\\u0b9f\\u0ba3\\u0ba4\\u0ba8\\u0ba9\\u0baa\\u0bae\\u0baf\\u0bb0\\u0bb1\\u0bb2\\u0bb3\\u0bb4\\u0bb5\\u0bb6\\u0bb7\\u0bb8\\u0bb9\\u0bbe\\u0bbf\\u0bc0\\u0bc1\\u0bc2\\u0bc6\\u0bc7\\u0bc8\\u0bca\\u0bcb\\u0bcc\\u0bcd\\u0bd0\\u0bd7\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef\\u0bf0\\u0bf1\\u0bf2\\u0bf3\\u0bf4\\u0bf5\\u0bf6\\u0bf7\\u0bf8\\u0bf9\\u0bfa]+\"\n",
        "  processed_string = re.sub(regex_pattern, \"\", text)\n",
        "  processed_string = re.sub('([,.?!])', r' \\1', processed_string)\n",
        "  return processed_string.strip()\n",
        "\n",
        "# def tamil_punctuate(text):\n",
        "#   regex_pattern = r\"[^,.?! \\u0b82\\u0b83\\u0b85\\u0b86\\u0b87\\u0b88\\u0b89\\u0b8a\\u0b8e\\u0b8f\\u0b90\\u0b92\\u0b93\\u0b94\\u0b95\\u0b99\\u09b9a\\u0b9c\\u0b9e\\u0b9f\\u0ba3\\u0ba4\\u0ba8\\u0ba9\\u0baa\\u0bae\\u0baf\\u0bb0\\u0bb1\\u0bb2\\u0bb3\\u0bb4\\u0bb5\\u0bb6\\u0bb7\\u0bb8\\u0bb9\\u0bbe\\u0bbf\\u0bc0\\u0bc1\\u0bc2\\u0bc6\\u0bc7\\u0bc8\\u0bca\\u0bcb\\u0bcc\\u0bcd\\u0bd0\\u0bd7\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef\\u0bf0\\u0bf1\\u0bf2\\u0bf3\\u0bf4\\u0bf5\\u0bf6\\u0bf7\\u0bf8\\u0bf9\\u0bfa]+\"\n",
        "#   processed_string = re.sub(regex_pattern, \"\", text)\n",
        "#   processed_string = re.sub('([,.?!])', r' \\1', processed_string)\n",
        "#   return processed_string\n",
        "\n",
        "# def english_punctuate(text):\n",
        "#   text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
        "#   text = tf.strings.regex_replace(text, '[.,?!]', r' \\0 ')\n",
        "#   text = tf.strings.strip(text)\n",
        "#   return text\n",
        "\n",
        "# text = train_en_ta[1]+'@'\n",
        "# print(text)\n",
        "# text = punctuate(text)\n",
        "# print(text)\n",
        "# temp = tf.constant(text)\n",
        "# print(temp)\n",
        "# temp = unicode_normalize(temp)\n",
        "\n",
        "# text = train_ta_ta[1]\n",
        "# print(text)\n",
        "# text = tamil_punctuate(text)\n",
        "# print(text)\n",
        "# temp = tf.constant(text)\n",
        "# print(temp)\n",
        "# temp = unicode_normalize(temp)\n",
        "# print(temp)\n"
      ],
      "metadata": {
        "id": "WquqHqCzoGwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Adding START and END tokens\n",
        "\n",
        "This helps the model in understanding where the beginning and end of a sequence is. The model we will be making, operates on sequences of fixed length, so adding these tokens will help the model mark where the sentence begins or ends. \n",
        "\n",
        "Thus, improving model performance and translation quality."
      ],
      "metadata": {
        "id": "747O0NgDheq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Adding START and END Tokens to the sentence.\n",
        "\n",
        "# def start_end(text):\n",
        "\n",
        "#   text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "#   return text\n",
        "\n",
        "# # temp = tf.constant(train_ta_ta[1])\n",
        "# # print(temp)\n",
        "# # temp = start_end(temp)\n",
        "# # print(temp)"
      ],
      "metadata": {
        "id": "cX8-N3PxkMv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.5 Compiling these functions into one\n",
        "\n",
        "This is done so that we can pass one single argument for the standardization parameter in the tf.keras.layers.TextVectorization"
      ],
      "metadata": {
        "id": "8JuIiw7-2RwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def compile_normalize(text):\n",
        "  \n",
        "#   # some error with decoding when we make the TextVectorization layer\n",
        "#   text = punctuate(text.numpy().decode()[0])\n",
        "#   text = unicode_normalize(text)\n",
        "#   text = lowercase(text)\n",
        "#   text = start_end(text)\n",
        "#   return text\n"
      ],
      "metadata": {
        "id": "_Jhhn6ny2f5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Loading the dataset\n",
        "\n",
        "We will load the dataset into a pandas dataframe, so that we can easily apply the preprocessing functions"
      ],
      "metadata": {
        "id": "BAfv51VjpQrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'INPUT_COLUMN': train_hi, 'TARGET_COLUMN': train_ta})\n",
        "\n",
        "input_data = df['INPUT_COLUMN'].apply(lambda x : punctuate(x)).tolist()\n",
        "target_data = df['TARGET_COLUMN'].apply(lambda x : punctuate(x) + ' <eos>').tolist()\n",
        "target_input_data = df['TARGET_COLUMN'].apply(lambda x : '<sos> ' + punctuate(x)).tolist()\n",
        "\n",
        "print(input_data[:5])\n",
        "print(target_data[:5])\n",
        "print(target_input_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKRW_mp_pXyp",
        "outputId": "39bd4d8d-d082-42e6-b111-507d3d771c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['मतरिमणडल मतरिमडल न सीमा एव उतपाद शलक अधिनियम म ससो क उनमलन और विभिनन वसतओ और सवाओ पर अधिभार क सबध म जीएसटी वयवसथा को लाग करन क लिए सशोधन को अनमोदित किया परधानमतरी शरी नरदर मोदी की अधयकषता म कदरीय मतरिमडल न निमनलिखित परसतावो को मजरी परदान कर दी।', 'सीमा शलक अधिनियम , म सशोधन  .', 'सीमा शलक टरिफ अधिनियम , म सशोधन  .', 'कदरीय उतपाद शलक टरिफ अधिनियम अपील ,   .', 'मतरिमणडल कदरीय मतरिमडल न सटारटपस क लिए निधि सथापित करन को मजरी दी परधानमतरी शरी नरदर मोदी की अधयकषता म कदरीय मतरिमडल न सटारटपस क लिए निधि सथापित करन क लिए निमनलिखित परसतावो को अनमति परदान कर दी ह।']\n",
            "['முறையை அமலபடுததுவதறகு வதியாக , பலவேறு ரககு மறறும ேவைகளுககான மேல வரி மறறும கூடுதல வரியை நககும வகையில , ுஙகம மறறும கலால டடததில திருததஙகளைக கொணடுவர மததிய அமைரவை ஒபபுதல பிரதமர திரு .நரேநதிர மோடி தலைமையில மததிய அமைரவைக கூடடம நடைபெறறது . இதில , கழககாணும பரிநதுரைகளுககு அபபோது ஒபபுதல அளிககபபடடது . <eos>', 'மேறகணட பரிநதுரைகளால , கழககாணும பலனகள கிடைககும ுஙகஙகள டடம ல ஏ , பி பிரிவுகள ேரககபபடுகினறன . <eos>', 'ஏனெனில , இநத டடஙகளை ிறபபான முறையில ெயலபடுதத ுஙகததுறை அதிகாரிகளுககு அஙககாரம அளிககபபடுகிறது . <eos>', 'முறையை அமலபடுததுவதால , தேவையிலலாத பிரிவுகள நககபபடுகினறன . <eos>', 'மததிய அமைரவை தொழில தொடஙகுவோம திடடததுககான நிதிககு நிதியை உருவாககும பரிநதுரைககு மததிய அமைரவை ஒபபுதல கடநத ஆணடு ஜூன மாதததில ரூ . , கோடி நிதியுடன உருவாககபபடட தொழில தொடஙகுவோம இயககததிறகான நிதியததிறகு நிதி Fund of Funds of Startups  FFS திடடம தொடரபாக கொணடுவரபபடடுளள கழககாணும பரிநதுரைகளுககு பிரதமர திரு .நரேநதிர மோடி தலைமையில நடைபெறற மததிய அமைரவைக கூடடததில ஒபபுதல அளிககபபடடது . <eos>']\n",
            "['<sos> முறையை அமலபடுததுவதறகு வதியாக , பலவேறு ரககு மறறும ேவைகளுககான மேல வரி மறறும கூடுதல வரியை நககும வகையில , ுஙகம மறறும கலால டடததில திருததஙகளைக கொணடுவர மததிய அமைரவை ஒபபுதல பிரதமர திரு .நரேநதிர மோடி தலைமையில மததிய அமைரவைக கூடடம நடைபெறறது . இதில , கழககாணும பரிநதுரைகளுககு அபபோது ஒபபுதல அளிககபபடடது .', '<sos> மேறகணட பரிநதுரைகளால , கழககாணும பலனகள கிடைககும ுஙகஙகள டடம ல ஏ , பி பிரிவுகள ேரககபபடுகினறன .', '<sos> ஏனெனில , இநத டடஙகளை ிறபபான முறையில ெயலபடுதத ுஙகததுறை அதிகாரிகளுககு அஙககாரம அளிககபபடுகிறது .', '<sos> முறையை அமலபடுததுவதால , தேவையிலலாத பிரிவுகள நககபபடுகினறன .', '<sos> மததிய அமைரவை தொழில தொடஙகுவோம திடடததுககான நிதிககு நிதியை உருவாககும பரிநதுரைககு மததிய அமைரவை ஒபபுதல கடநத ஆணடு ஜூன மாதததில ரூ . , கோடி நிதியுடன உருவாககபபடட தொழில தொடஙகுவோம இயககததிறகான நிதியததிறகு நிதி Fund of Funds of Startups  FFS திடடம தொடரபாக கொணடுவரபபடடுளள கழககாணும பரிநதுரைகளுககு பிரதமர திரு .நரேநதிர மோடி தலைமையில நடைபெறற மததிய அமைரவைக கூடடததில ஒபபுதல அளிககபபடடது .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting to save memory\n",
        "\n",
        "del df"
      ],
      "metadata": {
        "id": "02W359PErdBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Tokenizing and Processing the text data\n",
        "\n",
        "--> Tokenize the data, i.e., to convert the text into sequence of integers.\n",
        "\n",
        "--> Extract the sequence of integers from the text\n",
        "\n",
        "--> Finding the maximum length of input and output sequences"
      ],
      "metadata": {
        "id": "vmaVgNeSrpqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAX_VOCAB_SIZE = 20000\n",
        "# EMBEDDING_DIM = 128\n",
        "# HIDDEN_DIM=1024 #512\n",
        "\n",
        "# BATCH_SIZE = 64  # Batch size for training.\n",
        "# EPOCHS = 10  # Number of epochs to train for.\n",
        "\n",
        "# ATTENTION_FUNC='general'\n",
        "\n",
        "# Making a tokenizer for the input texts and fit them\n",
        "\n",
        "tokenizer_inputs = Tokenizer(num_words = 20000, filters='')\n",
        "tokenizer_inputs.fit_on_texts(input_data)\n",
        "\n",
        "# Tokenize and transforming texts into sequence of integers\n",
        "\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_data)\n",
        "\n",
        "# Calculating the max length\n",
        "\n",
        "input_max_len = max(len(s) for s in input_sequences)\n",
        "print(f'The Maximum input length is {input_max_len}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19V59b-2rt6x",
        "outputId": "796aec27-95f4-4928-9081-451a95a84be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Maximum input length is 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How input tokenized text looks like\n",
        "\n",
        "print(input_data[0])\n",
        "print(input_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tniCjvE5tWYL",
        "outputId": "576580a3-8e9a-4198-e7da-17f3c443ccd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "मतरिमणडल मतरिमडल न सीमा एव उतपाद शलक अधिनियम म ससो क उनमलन और विभिनन वसतओ और सवाओ पर अधिभार क सबध म जीएसटी वयवसथा को लाग करन क लिए सशोधन को अनमोदित किया परधानमतरी शरी नरदर मोदी की अधयकषता म कदरीय मतरिमडल न निमनलिखित परसतावो को मजरी परदान कर दी।\n",
            "[446, 117, 10, 482, 24, 719, 952, 400, 2, 1, 2675, 5, 96, 769, 5, 451, 13, 7750, 1, 282, 2, 542, 289, 6, 293, 15, 1, 11, 414, 6, 2753, 21, 20, 23, 671, 54, 4, 186, 2, 108, 117, 10, 892, 2374, 6, 110, 94, 26, 379]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a tokenizer for the output and fit it to them, including the data with both <eos> and <sos> tags\n",
        "\n",
        "tokenizer_outputs = Tokenizer(num_words = 20000, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_data)\n",
        "tokenizer_outputs.fit_on_texts(target_input_data)\n",
        "\n",
        "# Tokenize and transform output texts to sequence of integers\n",
        "\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_data)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_input_data)\n",
        "\n",
        "# Finding the maximum length for target sequence\n",
        "\n",
        "target_max_len = max(len(s) for s in target_sequences)\n",
        "print(f'The Maximum target length is {target_max_len}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsJgzR64to2o",
        "outputId": "cf869edc-2668-433c-a969-e11b73401a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Maximum target length is 231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How target tokenized texts look like\n",
        "\n",
        "print(target_data[0])\n",
        "print(target_sequences[0])\n",
        "print(target_input_data[0])\n",
        "print(target_sequences_inputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLw9VCjgu_lx",
        "outputId": "c095490b-9664-4ab6-b755-75fd386dbdf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "முறையை அமலபடுததுவதறகு வதியாக , பலவேறு ரககு மறறும ேவைகளுககான மேல வரி மறறும கூடுதல வரியை நககும வகையில , ுஙகம மறறும கலால டடததில திருததஙகளைக கொணடுவர மததிய அமைரவை ஒபபுதல பிரதமர திரு .நரேநதிர மோடி தலைமையில மததிய அமைரவைக கூடடம நடைபெறறது . இதில , கழககாணும பரிநதுரைகளுககு அபபோது ஒபபுதல அளிககபபடடது . <eos>\n",
            "[711, 8625, 2445, 2, 29, 566, 5, 6375, 1036, 166, 5, 296, 3486, 6834, 35, 2, 5828, 5, 5993, 1855, 8626, 2344, 8, 51, 50, 7, 9, 285, 34, 203, 8, 341, 466, 723, 1, 165, 2, 3974, 947, 50, 596, 1, 3]\n",
            "<sos> முறையை அமலபடுததுவதறகு வதியாக , பலவேறு ரககு மறறும ேவைகளுககான மேல வரி மறறும கூடுதல வரியை நககும வகையில , ுஙகம மறறும கலால டடததில திருததஙகளைக கொணடுவர மததிய அமைரவை ஒபபுதல பிரதமர திரு .நரேநதிர மோடி தலைமையில மததிய அமைரவைக கூடடம நடைபெறறது . இதில , கழககாணும பரிநதுரைகளுககு அபபோது ஒபபுதல அளிககபபடடது .\n",
            "[4, 711, 8625, 2445, 2, 29, 566, 5, 6375, 1036, 166, 5, 296, 3486, 6834, 35, 2, 5828, 5, 5993, 1855, 8626, 2344, 8, 51, 50, 7, 9, 285, 34, 203, 8, 341, 466, 723, 1, 165, 2, 3974, 947, 50, 596, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Mapping to make vocabularies\n",
        "\n",
        "Using the tokenizer we have created previously we can retrieve the vocabularies, one to match word to integer (word2idx) and a second one to match the integer to the corresponding word (idx2word)."
      ],
      "metadata": {
        "id": "UazsRwGuvcrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the word to index mapping for input language\n",
        "\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print(f'Found {len(word2idx_inputs)} unique input tokens.')\n",
        "\n",
        "# get the word to index mapping for output language\n",
        "\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print(f'Found {len(word2idx_outputs)} unique output tokens.')\n",
        "\n",
        "# store number of output and input words for later\n",
        "# Adding 1 since indexing starting from 1\n",
        "\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "num_words_inputs = len(word2idx_inputs) + 1\n",
        "\n",
        "# map indexes back into real words so that we can view the results\n",
        "\n",
        "idx2word_inputs = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_outputs = {v:k for k, v in word2idx_outputs.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjjjWV9VvmJQ",
        "outputId": "9c2b1bd1-3a3a-45fd-8d7f-b837ff868a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 45894 unique input tokens.\n",
            "Found 116293 unique output tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Padding the sentences \n",
        "\n",
        "We need to pad zeroes at the end of the sequences so that all the sequences have the same length, for training."
      ],
      "metadata": {
        "id": "JIaT8N4EwEbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pad the input sequences\n",
        "\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=input_max_len, padding='post')\n",
        "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
        "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
        "\n",
        "# pad the decoder input sequences\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=target_max_len, padding='post')\n",
        "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
        "\n",
        "# pad the target output sequences\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=target_max_len, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOfNyoKJwSh3",
        "outputId": "2c2e81a5-6dd0-4b46-ba5e-cf53b2fcf748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_inputs.shape: (73355, 256)\n",
            "encoder_inputs[0]: [ 446  117   10  482   24  719  952  400    2    1 2675    5   96  769\n",
            "    5  451   13 7750    1  282    2  542  289    6  293   15    1   11\n",
            "  414    6 2753   21   20   23  671   54    4  186    2  108  117   10\n",
            "  892 2374    6  110   94   26  379    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "decoder_inputs[0]: [   4  711 8625 2445    2   29  566    5 6375 1036  166    5  296 3486\n",
            " 6834   35    2 5828    5 5993 1855 8626 2344    8   51   50    7    9\n",
            "  285   34  203    8  341  466  723    1  165    2 3974  947   50  596\n",
            "    1    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0]\n",
            "decoder_inputs.shape: (73355, 231)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Making Batch Data generator\n",
        "\n",
        "We will train the model on batches, i.e., group of sentences, so we need to create a Dataset using the tf.data library and the function batch_on_slices on the input and output sequences."
      ],
      "metadata": {
        "id": "ziI6UdMswuwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((encoder_inputs, decoder_inputs, decoder_targets))\n",
        "dataset = dataset.shuffle(len(input_data)).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "rKCiT1vIw6kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Making the Model\n",
        "\n",
        "We will be making an Encoder-Decoder model with RNNs (Recurrent Neural Networks)\n",
        "\n",
        "--> Encoder: Layers of recurrent units where in each time step, receive an input token, collects relevant information and produce a hidden state. Depending on the type of RNN, in our example an LSTM, the unit \"mixes\" the current hidden state and the input and return an output, discarded, and a new hidden state.\n",
        "\n",
        "--> Encoder Vector: It is the last hidden state of the encoder and it tries to contain as much of the useful input information as possible to help the decoder get the best results. It is only information from the input that the decoder will get.\n",
        "\n",
        "--> Decoder: Layers of recurrent units, i.e. LSTMs, where each unit produces an output at a time step 't'. The hidden state of the first unit is the encoder vector and the rest of units accept the hidden state from the previous unit. The output is calculated using a softmax function to obtain a probability for every token in the output vocabulary."
      ],
      "metadata": {
        "id": "65Cbssq9h0pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Making the Encoder class"
      ],
      "metadata": {
        "id": "kXSoUMgvx3Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        \n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Define the embedding layer\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Define the RNN layer, LSTM\n",
        "\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            hidden_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, input_sequence, states):\n",
        "        \n",
        "        # Embed the input\n",
        "\n",
        "        embed = self.embedding(input_sequence)\n",
        "\n",
        "        # Call the LSTM unit\n",
        "\n",
        "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def init_states(self, batch_size):\n",
        "        \n",
        "        # Return all 0s initial states\n",
        "\n",
        "        return (tf.zeros([batch_size, self.hidden_dim]), tf.zeros([batch_size, self.hidden_dim]))"
      ],
      "metadata": {
        "id": "dixRhuCoh2AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Making the Decoder class"
      ],
      "metadata": {
        "id": "Nu8dAMdPyUQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Define the embedding layer\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Define the RNN layer, LSTM\n",
        "\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            hidden_dim, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, input_sequence, state):\n",
        "\n",
        "        # Embed the input\n",
        "\n",
        "        embed = self.embedding(input_sequence)\n",
        "\n",
        "        # Call the LSTM unit\n",
        "\n",
        "        lstm_out, state_h, state_c = self.lstm(embed, state)\n",
        "\n",
        "        # Dense layer to predict output token\n",
        "\n",
        "        logits = self.dense(lstm_out)\n",
        "        return logits, state_h, state_c"
      ],
      "metadata": {
        "id": "Bm1y3kngyXRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if its fine by initialising the Encoder and Decoder and set the initial hidden state"
      ],
      "metadata": {
        "id": "JvA1AKX4zgzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting embedding and hidden dimension\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_dim = 1024\n",
        "\n",
        "# Set the length of the input and output vocabulary\n",
        "\n",
        "num_words_inputs = len(word2idx_inputs) + 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "# Create the encoder\n",
        "\n",
        "encoder = Encoder(num_words_inputs, embedding_dim, hidden_dim)\n",
        "\n",
        "# Get the initial states\n",
        "\n",
        "initial_state = encoder.init_states(1)\n",
        "\n",
        "# Call the encoder for testing\n",
        "\n",
        "test_encoder_output = encoder(tf.constant([[1, 23, 4, 5, 0, 0]]), initial_state)\n",
        "print(test_encoder_output[0].shape)\n",
        "\n",
        "# Create the decoder\n",
        "\n",
        "decoder = Decoder(num_words_output, embedding_dim, hidden_dim)\n",
        "\n",
        "# Get the initial states\n",
        "\n",
        "de_initial_state = test_encoder_output[1:]\n",
        "\n",
        "# Call the decoder for testing\n",
        "\n",
        "test_decoder_output = decoder(tf.constant([[1, 3, 5, 7, 9, 0, 0, 0]]), de_initial_state)\n",
        "print(test_decoder_output[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H4vrxstzoO6",
        "outputId": "9d267336-7ceb-40a8-b665-eee1e8e9d08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 6, 1024)\n",
            "(1, 8, 116294)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3 Creating custom loss and accuracy function\n",
        "\n",
        "Now we need to define a custom loss function to avoid taking into account the 0 values, padding values, when calculating the loss. And also we have to define a custom accuracy function for the same."
      ],
      "metadata": {
        "id": "K_hZtcBZ1YX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(targets, logits):\n",
        "\n",
        "    # SparseCateogricalCrossEntropy is a loss metric for long integer sequences.\n",
        "\n",
        "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # Mask padding values, they do not have to compute for loss\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "\n",
        "    # Calculate the loss value using the mask\n",
        "\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "    return loss\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "\n",
        "    # y_pred shape is batch_size, seq length, vocab size\n",
        "    # y_true shape is batch_size, seq length\n",
        "\n",
        "    pred_values = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\n",
        "    correct = K.cast(K.equal(y_true, pred_values), dtype='float32')\n",
        "\n",
        "    # 0 is padding, so we shouldn't include those\n",
        "\n",
        "    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "    n_correct = K.sum(mask * correct)\n",
        "    n_total = K.sum(mask)\n",
        "    return n_correct / n_total"
      ],
      "metadata": {
        "id": "KELkg-yd1kGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training the Model"
      ],
      "metadata": {
        "id": "tf61Xtgsh460"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> We will be training the model in batches\n",
        "\n",
        "--> We feed the input sequence (which is a sequence of integers), target sequences (both for output and input to the decoder), takes in the encoder's state and then returns the loss and accuracy calculated by the functions defined above.\n",
        "\n",
        "--> Teacher Forcing: It is an algorithm wherein RNN units learn their weights, by putting the observed sequence back into the RNN unit, thus making the model stay close to the ground truth sequence."
      ],
      "metadata": {
        "id": "3qFnozOfHOrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Creating step train function"
      ],
      "metadata": {
        "id": "oYnyuA9HJ0EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the encoder on batch input and the output from that is the encoded vector\n",
        "# Decoder's initial states are the set to the encoded vector.\n",
        "# Call the decoder, taking the right shifted target sequence as input. The output are the logits (the softmax function is applied in the loss function)\n",
        "# Calculate the loss and accuracy of the batch data and update the learnable parameters of the encoder and the decoder\n",
        "# Update the optimizer\n",
        "\n",
        "# tf.function is a tool that created data flow graphs from the code\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
        "\n",
        "    \"\"\"\n",
        "    A training step -> train a batch of the data and return the loss value reached\n",
        "    Input:\n",
        "        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            This is the input sequence\n",
        "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            This is the target sequence\n",
        "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            This is the input sequence to the decoder, where we will use 'Teacher Forcing'\n",
        "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
        "            Initial state of the encoder\n",
        "        - optimizer: a tf.keras.optimizers.\n",
        "    Output:\n",
        "        - loss: loss value\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Get the encoder outputs\n",
        "        en_outputs = encoder(input_seq, en_initial_states)\n",
        "\n",
        "        # Set the encoder and decoder states\n",
        "        en_states = en_outputs[1:]\n",
        "        de_states = en_states\n",
        "\n",
        "        # Get the encoder outputs\n",
        "        de_outputs = decoder(target_seq_in, de_states)\n",
        "\n",
        "        # Take the actual output\n",
        "        logits = de_outputs[0]\n",
        "\n",
        "        # Calculate the loss function\n",
        "        loss = loss_func(target_seq_out, logits)\n",
        "        acc = accuracy_fn(target_seq_out, logits)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # Calculate the gradients for the variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    \n",
        "    # Apply the gradients and update the optimizer\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss, acc"
      ],
      "metadata": {
        "id": "yk6lXiyYh6qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Creating the main train function\n",
        "\n",
        "The training would take long duration, so we will save the checkpoint of the model after every two epochs. Later, we can use the object to retrieve the latest checkpoint object and use it to make predictions."
      ],
      "metadata": {
        "id": "WB6bgGuDKEHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_train(encoder, decoder, dataset, n_epochs, batch_size, optimizer, checkpoint, checkpoint_prefix):\n",
        "    \n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "\n",
        "        # Get the initial time\n",
        "        start = time.time()\n",
        "\n",
        "        # Get the initial state for the encoder\n",
        "        en_initial_states = encoder.init_states(batch_size)\n",
        "\n",
        "        # For every batch data\n",
        "        for batch, (input_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "\n",
        "            # Train and get the loss value \n",
        "            loss, accuracy = train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer)\n",
        "        \n",
        "            if batch % 100 == 0:\n",
        "\n",
        "                # Store the loss and accuracy values\n",
        "                losses.append(loss)\n",
        "                accuracies.append(accuracy)\n",
        "                print(f'Epoch {e+1} Batch {batch} Loss {loss.numpy():.4f} Acc:{accuracy.numpy():.4f}')\n",
        "                \n",
        "        # saving (checkpoint) the model every 2 epochs\n",
        "        if (e + 1) % 2 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "        print(f'Time taken for 1 epoch {(time.time()-start):.4f} secs\\n')\n",
        "        \n",
        "    return losses, accuracies"
      ],
      "metadata": {
        "id": "k69kl2NLKLz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Defining the optimizer and training the model"
      ],
      "metadata": {
        "id": "iekGl0VuRTDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Adam optimizer and clips gradients by its norm\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "\n",
        "# checkpoint object helps us to save the model\n",
        "checkpoint_dir = './training_ckpt_seq2seq'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,encoder=encoder,decoder=decoder)\n",
        "\n",
        "EPOCHS = 5\n",
        "losses, accuracies = main_train(encoder, decoder, dataset, EPOCHS, BATCH_SIZE, optimizer, checkpoint, checkpoint_prefix)"
      ],
      "metadata": {
        "id": "p0LsiAljRSop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluating accuracy"
      ],
      "metadata": {
        "id": "EFw0qK8hh7kk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWjxSU3qiT_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}