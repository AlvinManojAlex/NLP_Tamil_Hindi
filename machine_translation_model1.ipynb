{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinManojAlex/NLP_Tamil_Hindi/blob/main/machine_translation_model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the GDrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3iJEHnlZOM7",
        "outputId": "4dbfc176-e794-420f-efee-9a7bf836a2e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Reading datasets collected"
      ],
      "metadata": {
        "id": "hxLuzo7xCMDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Reading the Hindi-English Parallel Corpus"
      ],
      "metadata": {
        "id": "mM5Y423i30Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_en_hi = []\n",
        "train_hi_hi = []\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_hi/cvit_train.en.txt', 'r') as file:\n",
        "  for line in file:\n",
        "\n",
        "    # Appending the stripped english sentences into the list\n",
        "    train_en_hi.append(line.strip())\n",
        "    \n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_hi/cvit_train.hi.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    \n",
        "    # Appending the stripped hindi sentences into the list\n",
        "    train_hi_hi.append(line.strip())\n",
        "\n",
        "\n",
        "print(f'{len(train_en_hi)} english lines read from PIB_English_Hindi Corpus')\n",
        "print(f'{len(train_hi_hi)} hindi lines read from PIB_English_Hindi Corpus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbCHbDF_gsNe",
        "outputId": "5673e2d5-db22-4229-b6e0-68a3344de1f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "266545 english lines read from PIB_English_Hindi Corpus\n",
            "266545 hindi lines read from PIB_English_Hindi Corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Reading the Tamil-English Parallel corpus"
      ],
      "metadata": {
        "id": "9TgR1W3x3rSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_en_ta = []\n",
        "train_ta_ta = []\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_ta/cvit_train.en.txt', 'r') as file:\n",
        "  for line in file:\n",
        "\n",
        "    # Appending the stripped english sentences into the list\n",
        "    train_en_ta.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_ta/cvit_train.ta.txt', 'r') as file:\n",
        "  for line in file:\n",
        "\n",
        "    # Appending the stripped tamil sentences into the list\n",
        "    train_ta_ta.append(line.strip())\n",
        "\n",
        "print(f'{len(train_en_ta)} english lines read from PIB_English_Tamil Corpus')\n",
        "print(f'{len(train_ta_ta)} tamil lines read from PIB_English_Tamil Corpus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATHKIYyHgWEN",
        "outputId": "f11b68f8-1667-418b-cf98-1b67837f4578"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115968 english lines read from PIB_English_Tamil Corpus\n",
            "115968 tamil lines read from PIB_English_Tamil Corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_hi_hi[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w62oBOsWLKXz",
        "outputId": "eaf13873-0a42-441b-a546-eed8a2234c22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "प्रधानमंत्री कार्यालय इंडिया टुडे कॉनक्लेव में प्रधान मंत्री के भाषण (वीडियो कान्फ्रसिंग के माध्यम से) का मूल पाठ सबसे पहले आप सभी को इस आयोजन के लिए बहुत-बहुत बधाई-शुभकामनाएं।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_ta_ta[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYZBHBsSLhUI",
        "outputId": "0a3fc2c0-e939-4f05-9b33-75a9b64793cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "முறையை அமல்படுத்துவதற்கு வசதியாக, பல்வேறு சரக்கு மற்றும் சேவைகளுக்கான மேல் வரி மற்றும் கூடுதல் வரியை நீக்கும் வகையில், சுங்கம் மற்றும் கலால் சட்டத்தில் திருத்தங்களைக் கொண்டுவர மத்திய அமைச்சரவை ஒப்புதல் பிரதமர் திரு.நரேந்திர மோடி தலைமையில் மத்திய அமைச்சரவைக் கூட்டம் நடைபெற்றது. இதில், கீழ்க்காணும் பரிந்துரைகளுக்கு அப்போது ஒப்புதல் அளிக்கப்பட்டது.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Correcting the lists so that it matches"
      ],
      "metadata": {
        "id": "aXT057cw0UHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamil-English corpus contains fewer sentences than the Hindi-English corpus. And to do the translation between Hindi and Tamil, we will be required to modify the corpus such that Hindi and Tamil sentences match up."
      ],
      "metadata": {
        "id": "6Qi4GZHQ09Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_hi = []\n",
        "# train_ta = []\n",
        "\n",
        "# for i in range(0, len(train_en_ta)):\n",
        "#   for j in range(0, len(train_en_hi)):\n",
        "    \n",
        "#     if train_en_ta[i] == train_en_hi[j]:\n",
        "#       train_ta.append(train_ta_ta[i])\n",
        "#       train_hi.append(train_hi_hi[j])\n",
        "#       break\n",
        "    \n",
        "#     else:\n",
        "#       continue\n",
        "\n",
        "# print(len(train_hi))\n",
        "# print(len(train_ta))"
      ],
      "metadata": {
        "id": "hzKFtJyN0aSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving this to a file as a new parallel corpus"
      ],
      "metadata": {
        "id": "7eoGArQ8VwzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.hi.txt', 'w+') as file:\n",
        "#   for x in train_hi:\n",
        "#     file.write(x+'\\n')\n",
        "\n",
        "# with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.ta.txt', 'w+') as file:\n",
        "#   for x in train_ta:\n",
        "#     file.write(x+'\\n')"
      ],
      "metadata": {
        "id": "UGg1jfqqVwRW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_hi = []\n",
        "train_ta = []\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.hi.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    train_hi.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.ta.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    train_ta.append(line.strip())"
      ],
      "metadata": {
        "id": "lG9NHfnekGxk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data preprocessing"
      ],
      "metadata": {
        "id": "Lq7Ff8nwhVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing `tensorflow-text` and `einops` (Einstein Inspired Notation)"
      ],
      "metadata": {
        "id": "iRlRJt8TheGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow package for text related operations and modules\n",
        "!pip install tensorflow-text\n",
        "\n",
        "# Installing einops for writing deep learning code better and more efficiently\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "1d6iiKhhhdat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ded8f92-0538-4199-b07d-4098d54a01a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (0.31.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.51.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (15.0.6.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.22.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (23.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (4.5.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (23.3.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.16.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the packages"
      ],
      "metadata": {
        "id": "TzqlrJ9Nifxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ],
      "metadata": {
        "id": "DtQEe-yiihmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every sentence should be treated as a `tf.string`, since we are trying to export this model as `tf.saved_model`."
      ],
      "metadata": {
        "id": "BMINoCNFkLuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Normalizing the sentences\n",
        "\n",
        "P.S. Initially training the model without removing the bracketed words from the hindi corpus"
      ],
      "metadata": {
        "id": "wK1VP-0dcgiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Unicode Normalization\n",
        "\n",
        "UNICODE Normalization is essential to maintain accuracy and efficiency in language translation models. For both Hindi and Tamil languages we will be using `NFC` (Normalization Form - C), which is a 'Canonical Decomposition followed by Canonical Composition'. This ensures that equivalent characters are represented in a consistent way.\n",
        "\n",
        "<b>Reference:</b>\n",
        "\n",
        "https://unicode.org/reports/tr15/"
      ],
      "metadata": {
        "id": "t4bKO6jVfWIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that takes in a tensor and normalizes it according to NFC and returns the text\n",
        "\n",
        "def unicode_normalize(text):\n",
        "  text = tf_text.normalize_utf8(text, 'NFC')\n",
        "  return text\n",
        "\n",
        "# temp = tf.constant(train_hi_hi[1])\n",
        "# unicode_normalize(temp)\n",
        "# print(temp)"
      ],
      "metadata": {
        "id": "FZRxrsPulWsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Converting sentence to lowercase\n",
        "\n",
        "This is done to eliminate ambiguity, since we are using English language as an intermediary so it is neccessary to convert the english corpus to its lowercase form so that the model will see 'Car' and 'car' as the same word."
      ],
      "metadata": {
        "id": "SOkWLCYkfvx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that takes in a tensor and converts it to lowercase\n",
        "\n",
        "def lowercase(text):\n",
        "  text = tf.strings.lower(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "Xe70toyIm81M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 Replacing some special characters\n",
        "\n",
        "Characters like `?`, `!`, `.`, `,` and ` ` &nbsp;must not be removed from the sentence, while the other special characters must be removed. This is done by using regex to filter out the unwanted characters. Hence, we have to make a regex that does not filter out the Hindi and Tamil characters.\n",
        "\n",
        "<b>References:</b>\n",
        "\n",
        "https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)#:~:text=Devanagari%20is%20a%20Unicode%20block,from%20the%201988%20ISCII%20standard\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tamil_(Unicode_block)\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, followed by keeping a white space between the punctuations."
      ],
      "metadata": {
        "id": "zUFxm6lTgJA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Hindi and Tamil characters using UNICODE and then including that in the regex\n",
        "\n",
        "# UNICODE for Hindi characters are stored continously, so we will use a loop to make our list of Hindi characters\n",
        "\n",
        "hindi_characters = 128\n",
        "hindi_unicode_shift = 0x0900\n",
        "\n",
        "hindi_alphabets = []\n",
        "\n",
        "for i in range(0, hindi_characters):\n",
        "  hindi_alphabets.append('\\\\u0'+hex(hindi_unicode_shift+i)[2:])\n",
        "\n",
        "# UNICODE for Tamil characters are not stored continuously since they have some reserved UNICODE characters in between, so we will manually add them to our list\n",
        "\n",
        "tamil_alphabets = ['\\\\u0b82', '\\\\u0b83', '\\\\u0b85', '\\\\u0b86', '\\\\u0b87', '\\\\u0b88', '\\\\u0b89', '\\\\u0b8a', '\\\\u0b8e', '\\\\u0b8f', '\\\\u0b90', '\\\\u0b92', '\\\\u0b93', \n",
        "                   '\\\\u0b94', '\\\\u0b95', '\\\\u0b99', '\\\\u09b9a', '\\\\u0b9c', '\\\\u0b9e', '\\\\u0b9f', '\\\\u0ba3', '\\\\u0ba4', '\\\\u0ba8', '\\\\u0ba9', '\\\\u0baa', '\\\\u0bae'\n",
        "                   '\\\\u0baf', '\\\\u0bb0', '\\\\u0bb1', '\\\\u0bb2', '\\\\u0bb3', '\\\\u0bb4', '\\\\u0bb5', '\\\\u0bb6', '\\\\u0bb7', '\\\\u0bb8', '\\\\u0bb9', '\\\\u0bbe', '\\\\u0bbf',\n",
        "                   '\\\\u0bc0', '\\\\u0bc1', '\\\\u0bc2', '\\\\u0bc6', '\\\\u0bc7', '\\\\u0bc8', '\\\\u0bca', '\\\\u0bcb', '\\\\u0bcc', '\\\\u0bcd', '\\\\u0bd0', '\\\\u0bd7', '\\\\u0be6',\n",
        "                   '\\\\u0be7', '\\\\u0be8', '\\\\u0be9', '\\\\u0bea', '\\\\u0beb', '\\\\u0bec', '\\\\u0bed', '\\\\u0bee', '\\\\u0bef', '\\\\u0bf0', '\\\\u0bf1', '\\\\u0bf2', '\\\\u0bf3',\n",
        "                   '\\\\u0bf4', '\\\\u0bf5', '\\\\u0bf6', '\\\\u0bf7', '\\\\u0bf8', '\\\\u0bf9', '\\\\u0bfa']"
      ],
      "metadata": {
        "id": "Dldo4bB4t4sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that takes in a tensor and keeps `?`, `!`, `.`, `,`, ` ` as such and replaces the other special characters with ``\n",
        "# After that a white space is kept between the 'chosen' punctuations\n",
        "# Account for regex with Hindi and Tamil\n",
        "# Also account for more than 1 whitespace being generated\n",
        "\n",
        "import re\n",
        "\n",
        "def punctuate(text):\n",
        "  regex_pattern = r\"[^,.?! A-Za-z\\u0900-\\u097F\\u0b82\\u0b83\\u0b85\\u0b86\\u0b87\\u0b88\\u0b89\\u0b8a\\u0b8e\\u0b8f\\u0b90\\u0b92\\u0b93\\u0b94\\u0b95\\u0b99\\u09b9a\\u0b9c\\u0b9e\\u0b9f\\u0ba3\\u0ba4\\u0ba8\\u0ba9\\u0baa\\u0bae\\u0baf\\u0bb0\\u0bb1\\u0bb2\\u0bb3\\u0bb4\\u0bb5\\u0bb6\\u0bb7\\u0bb8\\u0bb9\\u0bbe\\u0bbf\\u0bc0\\u0bc1\\u0bc2\\u0bc6\\u0bc7\\u0bc8\\u0bca\\u0bcb\\u0bcc\\u0bcd\\u0bd0\\u0bd7\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef\\u0bf0\\u0bf1\\u0bf2\\u0bf3\\u0bf4\\u0bf5\\u0bf6\\u0bf7\\u0bf8\\u0bf9\\u0bfa]+\"\n",
        "  processed_string = re.sub(regex_pattern, \"\", text)\n",
        "  processed_string = re.sub('([,.?!])', r' \\1', processed_string)\n",
        "  return tf.constant(processed_string)\n",
        "\n",
        "# def tamil_punctuate(text):\n",
        "#   regex_pattern = r\"[^,.?! \\u0b82\\u0b83\\u0b85\\u0b86\\u0b87\\u0b88\\u0b89\\u0b8a\\u0b8e\\u0b8f\\u0b90\\u0b92\\u0b93\\u0b94\\u0b95\\u0b99\\u09b9a\\u0b9c\\u0b9e\\u0b9f\\u0ba3\\u0ba4\\u0ba8\\u0ba9\\u0baa\\u0bae\\u0baf\\u0bb0\\u0bb1\\u0bb2\\u0bb3\\u0bb4\\u0bb5\\u0bb6\\u0bb7\\u0bb8\\u0bb9\\u0bbe\\u0bbf\\u0bc0\\u0bc1\\u0bc2\\u0bc6\\u0bc7\\u0bc8\\u0bca\\u0bcb\\u0bcc\\u0bcd\\u0bd0\\u0bd7\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef\\u0bf0\\u0bf1\\u0bf2\\u0bf3\\u0bf4\\u0bf5\\u0bf6\\u0bf7\\u0bf8\\u0bf9\\u0bfa]+\"\n",
        "#   processed_string = re.sub(regex_pattern, \"\", text)\n",
        "#   processed_string = re.sub('([,.?!])', r' \\1', processed_string)\n",
        "#   return processed_string\n",
        "\n",
        "# def english_punctuate(text):\n",
        "#   text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
        "#   text = tf.strings.regex_replace(text, '[.,?!]', r' \\0 ')\n",
        "#   text = tf.strings.strip(text)\n",
        "#   return text\n",
        "\n",
        "# text = train_en_ta[1]+'@'\n",
        "# print(text)\n",
        "# text = punctuate(text)\n",
        "# print(text)\n",
        "# temp = tf.constant(text)\n",
        "# print(temp)\n",
        "# temp = unicode_normalize(temp)\n",
        "\n",
        "# text = train_ta_ta[1]\n",
        "# print(text)\n",
        "# text = tamil_punctuate(text)\n",
        "# print(text)\n",
        "# temp = tf.constant(text)\n",
        "# print(temp)\n",
        "# temp = unicode_normalize(temp)\n",
        "# print(temp)\n"
      ],
      "metadata": {
        "id": "WquqHqCzoGwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Adding START and END tokens\n",
        "\n",
        "This helps the model in understanding where the beginning and end of a sequence is. The model we will be making, operates on sequences of fixed length, so adding these tokens will help the model mark where the sentence begins or ends. \n",
        "\n",
        "Thus, improving model performance and translation quality."
      ],
      "metadata": {
        "id": "747O0NgDheq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding START and END Tokens to the sentence.\n",
        "\n",
        "def start_end(text):\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n",
        "\n",
        "# temp = tf.constant(train_ta_ta[1])\n",
        "# print(temp)\n",
        "# temp = start_end(temp)\n",
        "# print(temp)"
      ],
      "metadata": {
        "id": "cX8-N3PxkMv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.5 Compiling these functions into one\n",
        "\n",
        "This is done so that we can pass one single argument for the standardization parameter in the tf.keras.layers.TextVectorization"
      ],
      "metadata": {
        "id": "8JuIiw7-2RwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_normalize(text):\n",
        "  \n",
        "  # some error with decoding when we make the TextVectorization layer\n",
        "  text = punctuate(text.numpy().decode()[0])\n",
        "  text = unicode_normalize(text)\n",
        "  text = lowercase(text)\n",
        "  text = start_end(text)\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "_Jhhn6ny2f5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Preparing Dataset for model\n",
        "\n",
        "Here, we convert the lists of sentences into numpy arrays."
      ],
      "metadata": {
        "id": "wAXhai-1nh8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 16,03,080\n",
        "\n",
        "# try:\n",
        "#   chunked_list = np.array_split(train_hi_hi, 1000000)\n",
        "#   arrays = [np.array(chunk) for chunk in chunked_list]\n",
        "#   hi_hi_raw = np.concatenate(arrays)\n",
        "# except MemoryError:\n",
        "#   print('Out of memory')"
      ],
      "metadata": {
        "id": "u16kERPQn7O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Creating tf.data.Dataset"
      ],
      "metadata": {
        "id": "5oRS3FP0yaTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tf.data.Dataset for Hindi-English\n",
        "\n",
        "BUFFER_SIZE = len(train_hi_hi)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "hi_train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_hi_hi, train_en_hi))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "DXp2qmHVyfGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tf.data.Dataset for English-Tamil\n",
        "\n",
        "BUFFER_SIZE = len(train_en_ta)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "ta_train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_en_ta, train_ta_ta))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "FwAMwm9lzdXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Making the TextVectorization layer\n",
        "\n",
        "This layer will contain the function `compile_normalize`, and will handle the vocabulary extraction and conversion of input text into sequence of tokens."
      ],
      "metadata": {
        "id": "HrVQLPwx1qWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "hi_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=compile_normalize,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)"
      ],
      "metadata": {
        "id": "9xhA4cwK2B23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "jJfTh3l1-LGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Making the Model"
      ],
      "metadata": {
        "id": "65Cbssq9h0pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YYsWf_wPjHIs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dixRhuCoh2AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training the Model"
      ],
      "metadata": {
        "id": "tf61Xtgsh460"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yk6lXiyYh6qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluating accuracy"
      ],
      "metadata": {
        "id": "EFw0qK8hh7kk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWjxSU3qiT_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}