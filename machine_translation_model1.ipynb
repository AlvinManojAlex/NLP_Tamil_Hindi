{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinManojAlex/NLP_Tamil_Hindi/blob/main/machine_translation_model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the GDrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3iJEHnlZOM7",
        "outputId": "c8772703-7c70-4706-8cd8-ac891c029070"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Reading datasets collected"
      ],
      "metadata": {
        "id": "hxLuzo7xCMDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Reading the Hindi-English Parallel Corpus"
      ],
      "metadata": {
        "id": "mM5Y423i30Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_en_hi = []\n",
        "train_hi_hi = []\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_hi/cvit_train.en.txt', 'r') as file:\n",
        "  for line in file:\n",
        "\n",
        "    # Appending the stripped english sentences into the list\n",
        "    train_en_hi.append(line.strip())\n",
        "    \n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_hi/cvit_train.hi.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    \n",
        "    # Appending the stripped hindi sentences into the list\n",
        "    train_hi_hi.append(line.strip())\n",
        "\n",
        "\n",
        "print(f'{len(train_en_hi)} english lines read from PIB_English_Hindi Corpus')\n",
        "print(f'{len(train_hi_hi)} hindi lines read from PIB_English_Hindi Corpus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbCHbDF_gsNe",
        "outputId": "b8c2014c-fa13-4082-8cfd-76dc6db865f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "266545 english lines read from PIB_English_Hindi Corpus\n",
            "266545 hindi lines read from PIB_English_Hindi Corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Reading the Tamil-English Parallel corpus"
      ],
      "metadata": {
        "id": "9TgR1W3x3rSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_en_ta = []\n",
        "train_ta_ta = []\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_ta/cvit_train.en.txt', 'r') as file:\n",
        "  for line in file:\n",
        "\n",
        "    # Appending the stripped english sentences into the list\n",
        "    train_en_ta.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/en_ta/cvit_train.ta.txt', 'r') as file:\n",
        "  for line in file:\n",
        "\n",
        "    # Appending the stripped tamil sentences into the list\n",
        "    train_ta_ta.append(line.strip())\n",
        "\n",
        "print(f'{len(train_en_ta)} english lines read from PIB_English_Tamil Corpus')\n",
        "print(f'{len(train_ta_ta)} tamil lines read from PIB_English_Tamil Corpus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATHKIYyHgWEN",
        "outputId": "8506a743-1f9b-4fb0-cdcb-da30bc9e6ea3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115968 english lines read from PIB_English_Tamil Corpus\n",
            "115968 tamil lines read from PIB_English_Tamil Corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_hi_hi[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w62oBOsWLKXz",
        "outputId": "6377f1d3-69de-4ed4-8b56-66e53e3e955d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "प्रधानमंत्री कार्यालय इंडिया टुडे कॉनक्लेव में प्रधान मंत्री के भाषण (वीडियो कान्फ्रसिंग के माध्यम से) का मूल पाठ सबसे पहले आप सभी को इस आयोजन के लिए बहुत-बहुत बधाई-शुभकामनाएं।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_ta_ta[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYZBHBsSLhUI",
        "outputId": "7024f5fd-ef05-49f5-ebe8-75143b747506"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "முறையை அமல்படுத்துவதற்கு வசதியாக, பல்வேறு சரக்கு மற்றும் சேவைகளுக்கான மேல் வரி மற்றும் கூடுதல் வரியை நீக்கும் வகையில், சுங்கம் மற்றும் கலால் சட்டத்தில் திருத்தங்களைக் கொண்டுவர மத்திய அமைச்சரவை ஒப்புதல் பிரதமர் திரு.நரேந்திர மோடி தலைமையில் மத்திய அமைச்சரவைக் கூட்டம் நடைபெற்றது. இதில், கீழ்க்காணும் பரிந்துரைகளுக்கு அப்போது ஒப்புதல் அளிக்கப்பட்டது.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Correcting the lists so that it matches"
      ],
      "metadata": {
        "id": "aXT057cw0UHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamil-English corpus contains fewer sentences than the Hindi-English corpus. And to do the translation between Hindi and Tamil, we will be required to modify the corpus such that Hindi and Tamil sentences match up."
      ],
      "metadata": {
        "id": "6Qi4GZHQ09Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_hi = []\n",
        "# train_ta = []\n",
        "\n",
        "# for i in range(0, len(train_en_ta)):\n",
        "#   for j in range(0, len(train_en_hi)):\n",
        "    \n",
        "#     if train_en_ta[i] == train_en_hi[j]:\n",
        "#       train_ta.append(train_ta_ta[i])\n",
        "#       train_hi.append(train_hi_hi[j])\n",
        "#       break\n",
        "    \n",
        "#     else:\n",
        "#       continue\n",
        "\n",
        "# print(len(train_hi))\n",
        "# print(len(train_ta))"
      ],
      "metadata": {
        "id": "hzKFtJyN0aSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving this to a file as a new parallel corpus"
      ],
      "metadata": {
        "id": "7eoGArQ8VwzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.hi.txt', 'w+') as file:\n",
        "#   for x in train_hi:\n",
        "#     file.write(x+'\\n')\n",
        "\n",
        "# with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.ta.txt', 'w+') as file:\n",
        "#   for x in train_ta:\n",
        "#     file.write(x+'\\n')"
      ],
      "metadata": {
        "id": "UGg1jfqqVwRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_hi = []\n",
        "train_ta = []\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.hi.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    train_hi.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/corpus/train/hi_ta/cvit_train.ta.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    train_ta.append(line.strip())"
      ],
      "metadata": {
        "id": "lG9NHfnekGxk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_hi[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liviu8gdrO0A",
        "outputId": "7ad8c949-cf52-4d73-a789-3b95ac5c8d0f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "मंत्रिमण्डल मंत्रिमंडल ने सीमा एवं उत्पाद शुल्क अधिनियम में सेसों के उन्मूलन और विभिन्न वस्तुओं और सेवाओं पर अधिभार के संबंध में जीएसटी व्यवस्था को लागू करने के लिए संशोधन को अनुमोदित किया प्रधानमंत्री श्री नरेंद्र मोदी की अध्यक्षता में केंद्रीय मंत्रिमंडल ने निम्नलिखित प्रस्तावों को मंजूरी प्रदान कर दी।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data preprocessing"
      ],
      "metadata": {
        "id": "Lq7Ff8nwhVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing `tensorflow-text` and `einops` (Einstein Inspired Notation)"
      ],
      "metadata": {
        "id": "iRlRJt8TheGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow package for text related operations and modules\n",
        "!pip install tensorflow-text\n",
        "\n",
        "# Installing einops for writing deep learning code better and more efficiently\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "1d6iiKhhhdat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749d2ab5-e0b8-41be-b594-668016250cbd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.11.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.22.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (0.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (23.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (3.19.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.11.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (15.0.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (4.5.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.51.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (63.4.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.27.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.16.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.2.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (6.0.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the packages"
      ],
      "metadata": {
        "id": "TzqlrJ9Nifxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "DtQEe-yiihmc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every sentence should be treated as a `tf.string`, since we are trying to export this model as `tf.saved_model`."
      ],
      "metadata": {
        "id": "BMINoCNFkLuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Normalizing the sentences\n",
        "\n",
        "P.S. Initially training the model without removing the bracketed words from the hindi corpus"
      ],
      "metadata": {
        "id": "wK1VP-0dcgiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Unicode Normalization\n",
        "\n",
        "UNICODE Normalization is essential to maintain accuracy and efficiency in language translation models. For both Hindi and Tamil languages we will be using `NFC` (Normalization Form - C), which is a 'Canonical Decomposition followed by Canonical Composition'. This ensures that equivalent characters are represented in a consistent way.\n",
        "\n",
        "<b>Reference:</b>\n",
        "\n",
        "https://unicode.org/reports/tr15/"
      ],
      "metadata": {
        "id": "t4bKO6jVfWIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that takes in a tensor and normalizes it according to NFC and returns the text\n",
        "\n",
        "def unicode_to_ascii(text):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# temp = tf.constant(train_hi_hi[1])\n",
        "# unicode_normalize(temp)\n",
        "# print(temp)"
      ],
      "metadata": {
        "id": "FZRxrsPulWsW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Converting sentence to lowercase\n",
        "\n",
        "This is done to eliminate ambiguity, since we are using English language as an intermediary so it is neccessary to convert the english corpus to its lowercase form so that the model will see 'Car' and 'car' as the same word."
      ],
      "metadata": {
        "id": "SOkWLCYkfvx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 Replacing some special characters\n",
        "\n",
        "Characters like `?`, `!`, `.`, `,` and ` ` &nbsp;must not be removed from the sentence, while the other special characters must be removed. This is done by using regex to filter out the unwanted characters. Hence, we have to make a regex that does not filter out the Hindi and Tamil characters.\n",
        "\n",
        "<b>References:</b>\n",
        "\n",
        "https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)#:~:text=Devanagari%20is%20a%20Unicode%20block,from%20the%201988%20ISCII%20standard\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tamil_(Unicode_block)\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, followed by keeping a white space between the punctuations."
      ],
      "metadata": {
        "id": "zUFxm6lTgJA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Hindi and Tamil characters using UNICODE and then including that in the regex\n",
        "\n",
        "# UNICODE for Hindi characters are stored continously, so we will use a loop to make our list of Hindi characters\n",
        "\n",
        "hindi_characters = 128\n",
        "hindi_unicode_shift = 0x0900\n",
        "\n",
        "hindi_alphabets = []\n",
        "\n",
        "for i in range(0, hindi_characters):\n",
        "  hindi_alphabets.append('\\\\u0'+hex(hindi_unicode_shift+i)[2:])\n",
        "\n",
        "# UNICODE for Tamil characters are not stored continuously since they have some reserved UNICODE characters in between, so we will manually add them to our list\n",
        "\n",
        "tamil_alphabets = ['\\\\u0b82', '\\\\u0b83', '\\\\u0b85', '\\\\u0b86', '\\\\u0b87', '\\\\u0b88', '\\\\u0b89', '\\\\u0b8a', '\\\\u0b8e', '\\\\u0b8f', '\\\\u0b90', '\\\\u0b92', '\\\\u0b93', \n",
        "                   '\\\\u0b94', '\\\\u0b95', '\\\\u0b99', '\\\\u09b9a', '\\\\u0b9c', '\\\\u0b9e', '\\\\u0b9f', '\\\\u0ba3', '\\\\u0ba4', '\\\\u0ba8', '\\\\u0ba9', '\\\\u0baa', '\\\\u0bae'\n",
        "                   '\\\\u0baf', '\\\\u0bb0', '\\\\u0bb1', '\\\\u0bb2', '\\\\u0bb3', '\\\\u0bb4', '\\\\u0bb5', '\\\\u0bb6', '\\\\u0bb7', '\\\\u0bb8', '\\\\u0bb9', '\\\\u0bbe', '\\\\u0bbf',\n",
        "                   '\\\\u0bc0', '\\\\u0bc1', '\\\\u0bc2', '\\\\u0bc6', '\\\\u0bc7', '\\\\u0bc8', '\\\\u0bca', '\\\\u0bcb', '\\\\u0bcc', '\\\\u0bcd', '\\\\u0bd0', '\\\\u0bd7', '\\\\u0be6',\n",
        "                   '\\\\u0be7', '\\\\u0be8', '\\\\u0be9', '\\\\u0bea', '\\\\u0beb', '\\\\u0bec', '\\\\u0bed', '\\\\u0bee', '\\\\u0bef', '\\\\u0bf0', '\\\\u0bf1', '\\\\u0bf2', '\\\\u0bf3',\n",
        "                   '\\\\u0bf4', '\\\\u0bf5', '\\\\u0bf6', '\\\\u0bf7', '\\\\u0bf8', '\\\\u0bf9', '\\\\u0bfa']"
      ],
      "metadata": {
        "id": "Dldo4bB4t4sR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that takes in a tensor and keeps `?`, `!`, `.`, `,`, ` ` as such and replaces the other special characters with ``\n",
        "# After that a white space is kept between the 'chosen' punctuations\n",
        "# Account for regex with Hindi and Tamil\n",
        "# Also account for more than 1 whitespace being generated\n",
        "\n",
        "import re\n",
        "\n",
        "def punctuate(text):\n",
        "  text = unicode_to_ascii(text)\n",
        "  regex_pattern = r\"[^,.?! A-Za-z\\u0900-\\u097F\\u0b82\\u0b83\\u0b85\\u0b86\\u0b87\\u0b88\\u0b89\\u0b8a\\u0b8e\\u0b8f\\u0b90\\u0b92\\u0b93\\u0b94\\u0b95\\u0b99\\u09b9a\\u0b9c\\u0b9e\\u0b9f\\u0ba3\\u0ba4\\u0ba8\\u0ba9\\u0baa\\u0bae\\u0baf\\u0bb0\\u0bb1\\u0bb2\\u0bb3\\u0bb4\\u0bb5\\u0bb6\\u0bb7\\u0bb8\\u0bb9\\u0bbe\\u0bbf\\u0bc0\\u0bc1\\u0bc2\\u0bc6\\u0bc7\\u0bc8\\u0bca\\u0bcb\\u0bcc\\u0bcd\\u0bd0\\u0bd7\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef\\u0bf0\\u0bf1\\u0bf2\\u0bf3\\u0bf4\\u0bf5\\u0bf6\\u0bf7\\u0bf8\\u0bf9\\u0bfa]+\"\n",
        "  processed_string = re.sub(regex_pattern, \"\", text)\n",
        "  processed_string = re.sub('([,.?!])', r' \\1', processed_string)\n",
        "  return processed_string.strip()\n",
        "\n",
        "# def tamil_punctuate(text):\n",
        "#   regex_pattern = r\"[^,.?! \\u0b82\\u0b83\\u0b85\\u0b86\\u0b87\\u0b88\\u0b89\\u0b8a\\u0b8e\\u0b8f\\u0b90\\u0b92\\u0b93\\u0b94\\u0b95\\u0b99\\u09b9a\\u0b9c\\u0b9e\\u0b9f\\u0ba3\\u0ba4\\u0ba8\\u0ba9\\u0baa\\u0bae\\u0baf\\u0bb0\\u0bb1\\u0bb2\\u0bb3\\u0bb4\\u0bb5\\u0bb6\\u0bb7\\u0bb8\\u0bb9\\u0bbe\\u0bbf\\u0bc0\\u0bc1\\u0bc2\\u0bc6\\u0bc7\\u0bc8\\u0bca\\u0bcb\\u0bcc\\u0bcd\\u0bd0\\u0bd7\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef\\u0bf0\\u0bf1\\u0bf2\\u0bf3\\u0bf4\\u0bf5\\u0bf6\\u0bf7\\u0bf8\\u0bf9\\u0bfa]+\"\n",
        "#   processed_string = re.sub(regex_pattern, \"\", text)\n",
        "#   processed_string = re.sub('([,.?!])', r' \\1', processed_string)\n",
        "#   return processed_string\n",
        "\n",
        "# def english_punctuate(text):\n",
        "#   text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
        "#   text = tf.strings.regex_replace(text, '[.,?!]', r' \\0 ')\n",
        "#   text = tf.strings.strip(text)\n",
        "#   return text\n",
        "\n",
        "# text = train_en_ta[1]+'@'\n",
        "# print(text)\n",
        "# text = punctuate(text)\n",
        "# print(text)\n",
        "# temp = tf.constant(text)\n",
        "# print(temp)\n",
        "# temp = unicode_normalize(temp)\n",
        "\n",
        "# text = train_ta_ta[1]\n",
        "# print(text)\n",
        "# text = tamil_punctuate(text)\n",
        "# print(text)\n",
        "# temp = tf.constant(text)\n",
        "# print(temp)\n",
        "# temp = unicode_normalize(temp)\n",
        "# print(temp)\n"
      ],
      "metadata": {
        "id": "WquqHqCzoGwu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Adding START and END tokens\n",
        "\n",
        "This helps the model in understanding where the beginning and end of a sequence is. The model we will be making, operates on sequences of fixed length, so adding these tokens will help the model mark where the sentence begins or ends. \n",
        "\n",
        "Thus, improving model performance and translation quality."
      ],
      "metadata": {
        "id": "747O0NgDheq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Adding START and END Tokens to the sentence.\n",
        "\n",
        "# def start_end(text):\n",
        "\n",
        "#   text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "#   return text\n",
        "\n",
        "# # temp = tf.constant(train_ta_ta[1])\n",
        "# # print(temp)\n",
        "# # temp = start_end(temp)\n",
        "# # print(temp)"
      ],
      "metadata": {
        "id": "cX8-N3PxkMv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.5 Compiling these functions into one\n",
        "\n",
        "This is done so that we can pass one single argument for the standardization parameter in the tf.keras.layers.TextVectorization"
      ],
      "metadata": {
        "id": "8JuIiw7-2RwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def compile_normalize(text):\n",
        "  \n",
        "#   # some error with decoding when we make the TextVectorization layer\n",
        "#   text = punctuate(text.numpy().decode()[0])\n",
        "#   text = unicode_normalize(text)\n",
        "#   text = lowercase(text)\n",
        "#   text = start_end(text)\n",
        "#   return text\n"
      ],
      "metadata": {
        "id": "_Jhhn6ny2f5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Loading the dataset\n",
        "\n",
        "We will load the dataset into a pandas dataframe, so that we can easily apply the preprocessing functions"
      ],
      "metadata": {
        "id": "BAfv51VjpQrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'INPUT_COLUMN': train_hi, 'TARGET_COLUMN': train_ta})\n",
        "\n",
        "input_data = df['INPUT_COLUMN'].apply(lambda x : punctuate(x)).tolist()\n",
        "target_data = df['TARGET_COLUMN'].apply(lambda x : punctuate(x) + ' <eos>').tolist()\n",
        "target_input_data = df['TARGET_COLUMN'].apply(lambda x : '<sos> ' + punctuate(x)).tolist()\n",
        "\n",
        "print(input_data[:5])\n",
        "print(target_data[:5])\n",
        "print(target_input_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKRW_mp_pXyp",
        "outputId": "23a410fe-cdcf-459d-ddac-27c776f6f42d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['मतरिमणडल मतरिमडल न सीमा एव उतपाद शलक अधिनियम म ससो क उनमलन और विभिनन वसतओ और सवाओ पर अधिभार क सबध म जीएसटी वयवसथा को लाग करन क लिए सशोधन को अनमोदित किया परधानमतरी शरी नरदर मोदी की अधयकषता म कदरीय मतरिमडल न निमनलिखित परसतावो को मजरी परदान कर दी।', 'सीमा शलक अधिनियम , म सशोधन  .', 'सीमा शलक टरिफ अधिनियम , म सशोधन  .', 'कदरीय उतपाद शलक टरिफ अधिनियम अपील ,   .', 'मतरिमणडल कदरीय मतरिमडल न सटारटपस क लिए निधि सथापित करन को मजरी दी परधानमतरी शरी नरदर मोदी की अधयकषता म कदरीय मतरिमडल न सटारटपस क लिए निधि सथापित करन क लिए निमनलिखित परसतावो को अनमति परदान कर दी ह।']\n",
            "['முறையை அமலபடுததுவதறகு வதியாக , பலவேறு ரககு மறறும ேவைகளுககான மேல வரி மறறும கூடுதல வரியை நககும வகையில , ுஙகம மறறும கலால டடததில திருததஙகளைக கொணடுவர மததிய அமைரவை ஒபபுதல பிரதமர திரு .நரேநதிர மோடி தலைமையில மததிய அமைரவைக கூடடம நடைபெறறது . இதில , கழககாணும பரிநதுரைகளுககு அபபோது ஒபபுதல அளிககபபடடது . <eos>', 'மேறகணட பரிநதுரைகளால , கழககாணும பலனகள கிடைககும ுஙகஙகள டடம ல ஏ , பி பிரிவுகள ேரககபபடுகினறன . <eos>', 'ஏனெனில , இநத டடஙகளை ிறபபான முறையில ெயலபடுதத ுஙகததுறை அதிகாரிகளுககு அஙககாரம அளிககபபடுகிறது . <eos>', 'முறையை அமலபடுததுவதால , தேவையிலலாத பிரிவுகள நககபபடுகினறன . <eos>', 'மததிய அமைரவை தொழில தொடஙகுவோம திடடததுககான நிதிககு நிதியை உருவாககும பரிநதுரைககு மததிய அமைரவை ஒபபுதல கடநத ஆணடு ஜூன மாதததில ரூ . , கோடி நிதியுடன உருவாககபபடட தொழில தொடஙகுவோம இயககததிறகான நிதியததிறகு நிதி Fund of Funds of Startups  FFS திடடம தொடரபாக கொணடுவரபபடடுளள கழககாணும பரிநதுரைகளுககு பிரதமர திரு .நரேநதிர மோடி தலைமையில நடைபெறற மததிய அமைரவைக கூடடததில ஒபபுதல அளிககபபடடது . <eos>']\n",
            "['<sos> முறையை அமலபடுததுவதறகு வதியாக , பலவேறு ரககு மறறும ேவைகளுககான மேல வரி மறறும கூடுதல வரியை நககும வகையில , ுஙகம மறறும கலால டடததில திருததஙகளைக கொணடுவர மததிய அமைரவை ஒபபுதல பிரதமர திரு .நரேநதிர மோடி தலைமையில மததிய அமைரவைக கூடடம நடைபெறறது . இதில , கழககாணும பரிநதுரைகளுககு அபபோது ஒபபுதல அளிககபபடடது .', '<sos> மேறகணட பரிநதுரைகளால , கழககாணும பலனகள கிடைககும ுஙகஙகள டடம ல ஏ , பி பிரிவுகள ேரககபபடுகினறன .', '<sos> ஏனெனில , இநத டடஙகளை ிறபபான முறையில ெயலபடுதத ுஙகததுறை அதிகாரிகளுககு அஙககாரம அளிககபபடுகிறது .', '<sos> முறையை அமலபடுததுவதால , தேவையிலலாத பிரிவுகள நககபபடுகினறன .', '<sos> மததிய அமைரவை தொழில தொடஙகுவோம திடடததுககான நிதிககு நிதியை உருவாககும பரிநதுரைககு மததிய அமைரவை ஒபபுதல கடநத ஆணடு ஜூன மாதததில ரூ . , கோடி நிதியுடன உருவாககபபடட தொழில தொடஙகுவோம இயககததிறகான நிதியததிறகு நிதி Fund of Funds of Startups  FFS திடடம தொடரபாக கொணடுவரபபடடுளள கழககாணும பரிநதுரைகளுககு பிரதமர திரு .நரேநதிர மோடி தலைமையில நடைபெறற மததிய அமைரவைக கூடடததில ஒபபுதல அளிககபபடடது .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting to save memory\n",
        "\n",
        "del df"
      ],
      "metadata": {
        "id": "02W359PErdBB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Tokenizing and Processing the text data\n",
        "\n",
        "--> Tokenize the data, i.e., to convert the text into sequence of integers.\n",
        "\n",
        "--> Extract the sequence of integers from the text\n",
        "\n",
        "--> Finding the maximum length of input and output sequences"
      ],
      "metadata": {
        "id": "vmaVgNeSrpqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAX_VOCAB_SIZE = 20000\n",
        "# EMBEDDING_DIM = 128\n",
        "# HIDDEN_DIM=1024 #512\n",
        "\n",
        "# BATCH_SIZE = 64  # Batch size for training.\n",
        "# EPOCHS = 10  # Number of epochs to train for.\n",
        "\n",
        "# ATTENTION_FUNC='general'\n",
        "\n",
        "# Making a tokenizer for the input texts and fit them\n",
        "\n",
        "tokenizer_inputs = Tokenizer(num_words = 20000, filters='')\n",
        "tokenizer_inputs.fit_on_texts(input_data)\n",
        "\n",
        "# Tokenize and transforming texts into sequence of integers\n",
        "\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_data)\n",
        "\n",
        "# Calculating the max length\n",
        "\n",
        "input_max_len = max(len(s) for s in input_sequences)\n",
        "print(f'The Maximum input length is {input_max_len}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19V59b-2rt6x",
        "outputId": "0831d8f7-43bd-490f-871b-345104141545"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Maximum input length is 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How input tokenized text looks like\n",
        "\n",
        "print(input_data[0])\n",
        "print(input_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tniCjvE5tWYL",
        "outputId": "9ed4875b-3820-48f3-88aa-d311cc31f511"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "मतरिमणडल मतरिमडल न सीमा एव उतपाद शलक अधिनियम म ससो क उनमलन और विभिनन वसतओ और सवाओ पर अधिभार क सबध म जीएसटी वयवसथा को लाग करन क लिए सशोधन को अनमोदित किया परधानमतरी शरी नरदर मोदी की अधयकषता म कदरीय मतरिमडल न निमनलिखित परसतावो को मजरी परदान कर दी।\n",
            "[446, 117, 10, 482, 24, 719, 952, 400, 2, 1, 2675, 5, 96, 769, 5, 451, 13, 7750, 1, 282, 2, 542, 289, 6, 293, 15, 1, 11, 414, 6, 2753, 21, 20, 23, 671, 54, 4, 186, 2, 108, 117, 10, 892, 2374, 6, 110, 94, 26, 379]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a tokenizer for the output and fit it to them, including the data with both <eos> and <sos> tags\n",
        "\n",
        "tokenizer_outputs = Tokenizer(num_words = 20000, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_data)\n",
        "tokenizer_outputs.fit_on_texts(target_input_data)\n",
        "\n",
        "# Tokenize and transform output texts to sequence of integers\n",
        "\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_data)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_input_data)\n",
        "\n",
        "# Finding the maximum length for target sequence\n",
        "\n",
        "target_max_len = max(len(s) for s in target_sequences)\n",
        "print(f'The Maximum target length is {target_max_len}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsJgzR64to2o",
        "outputId": "53dc6af0-e5ad-4a5d-e361-a9a24a3058f3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Maximum target length is 231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How target tokenized texts look like\n",
        "\n",
        "print(target_data[0])\n",
        "print(target_sequences[0])\n",
        "print(target_input_data[0])\n",
        "print(target_sequences_inputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLw9VCjgu_lx",
        "outputId": "1b8819b7-6b3c-4492-8c55-06efb4951c23"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "முறையை அமலபடுததுவதறகு வதியாக , பலவேறு ரககு மறறும ேவைகளுககான மேல வரி மறறும கூடுதல வரியை நககும வகையில , ுஙகம மறறும கலால டடததில திருததஙகளைக கொணடுவர மததிய அமைரவை ஒபபுதல பிரதமர திரு .நரேநதிர மோடி தலைமையில மததிய அமைரவைக கூடடம நடைபெறறது . இதில , கழககாணும பரிநதுரைகளுககு அபபோது ஒபபுதல அளிககபபடடது . <eos>\n",
            "[711, 8625, 2445, 2, 29, 566, 5, 6375, 1036, 166, 5, 296, 3486, 6834, 35, 2, 5828, 5, 5993, 1855, 8626, 2344, 8, 51, 50, 7, 9, 285, 34, 203, 8, 341, 466, 723, 1, 165, 2, 3974, 947, 50, 596, 1, 3]\n",
            "<sos> முறையை அமலபடுததுவதறகு வதியாக , பலவேறு ரககு மறறும ேவைகளுககான மேல வரி மறறும கூடுதல வரியை நககும வகையில , ுஙகம மறறும கலால டடததில திருததஙகளைக கொணடுவர மததிய அமைரவை ஒபபுதல பிரதமர திரு .நரேநதிர மோடி தலைமையில மததிய அமைரவைக கூடடம நடைபெறறது . இதில , கழககாணும பரிநதுரைகளுககு அபபோது ஒபபுதல அளிககபபடடது .\n",
            "[4, 711, 8625, 2445, 2, 29, 566, 5, 6375, 1036, 166, 5, 296, 3486, 6834, 35, 2, 5828, 5, 5993, 1855, 8626, 2344, 8, 51, 50, 7, 9, 285, 34, 203, 8, 341, 466, 723, 1, 165, 2, 3974, 947, 50, 596, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Mapping to make vocabularies\n",
        "\n",
        "Using the tokenizer we have created previously we can retrieve the vocabularies, one to match word to integer (word2idx) and a second one to match the integer to the corresponding word (idx2word)."
      ],
      "metadata": {
        "id": "UazsRwGuvcrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the word to index mapping for input language\n",
        "\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print(f'Found {len(word2idx_inputs)} unique input tokens.')\n",
        "\n",
        "# get the word to index mapping for output language\n",
        "\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print(f'Found {len(word2idx_outputs)} unique output tokens.')\n",
        "\n",
        "# store number of output and input words for later\n",
        "# Adding 1 since indexing starting from 1\n",
        "\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "num_words_inputs = len(word2idx_inputs) + 1\n",
        "\n",
        "# map indexes back into real words so that we can view the results\n",
        "\n",
        "idx2word_inputs = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_outputs = {v:k for k, v in word2idx_outputs.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjjjWV9VvmJQ",
        "outputId": "d9fcbc8d-02ce-4ef1-ca2e-8f418131026f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 45894 unique input tokens.\n",
            "Found 116293 unique output tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Padding the sentences \n",
        "\n",
        "We need to pad zeroes at the end of the sequences so that all the sequences have the same length, for training."
      ],
      "metadata": {
        "id": "JIaT8N4EwEbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pad the input sequences\n",
        "\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=input_max_len, padding='post')\n",
        "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
        "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
        "\n",
        "# pad the decoder input sequences\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=target_max_len, padding='post')\n",
        "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
        "\n",
        "# pad the target output sequences\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=target_max_len, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOfNyoKJwSh3",
        "outputId": "62215cc0-58cc-4e3c-94dd-1f6a59bdb487"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_inputs.shape: (73355, 256)\n",
            "encoder_inputs[0]: [ 446  117   10  482   24  719  952  400    2    1 2675    5   96  769\n",
            "    5  451   13 7750    1  282    2  542  289    6  293   15    1   11\n",
            "  414    6 2753   21   20   23  671   54    4  186    2  108  117   10\n",
            "  892 2374    6  110   94   26  379    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "decoder_inputs[0]: [   4  711 8625 2445    2   29  566    5 6375 1036  166    5  296 3486\n",
            " 6834   35    2 5828    5 5993 1855 8626 2344    8   51   50    7    9\n",
            "  285   34  203    8  341  466  723    1  165    2 3974  947   50  596\n",
            "    1    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0]\n",
            "decoder_inputs.shape: (73355, 231)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Making Batch Data generator\n",
        "\n",
        "We will train the model on batches, i.e., group of sentences, so we need to create a Dataset using the tf.data library and the function batch_on_slices on the input and output sequences."
      ],
      "metadata": {
        "id": "ziI6UdMswuwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((encoder_inputs, decoder_inputs, decoder_targets))\n",
        "dataset = dataset.shuffle(len(input_data)).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "rKCiT1vIw6kS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Making the Model\n",
        "\n",
        "We will be making an Encoder-Decoder model with RNNs (Recurrent Neural Networks)\n",
        "\n",
        "--> Encoder: Layers of recurrent units where in each time step, receive an input token, collects relevant information and produce a hidden state. Depending on the type of RNN, in our example an LSTM, the unit \"mixes\" the current hidden state and the input and return an output, discarded, and a new hidden state.\n",
        "\n",
        "--> Encoder Vector: It is the last hidden state of the encoder and it tries to contain as much of the useful input information as possible to help the decoder get the best results. It is only information from the input that the decoder will get.\n",
        "\n",
        "--> Decoder: Layers of recurrent units, i.e. LSTMs, where each unit produces an output at a time step 't'. The hidden state of the first unit is the encoder vector and the rest of units accept the hidden state from the previous unit. The output is calculated using a softmax function to obtain a probability for every token in the output vocabulary."
      ],
      "metadata": {
        "id": "65Cbssq9h0pu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dixRhuCoh2AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training the Model"
      ],
      "metadata": {
        "id": "tf61Xtgsh460"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yk6lXiyYh6qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluating accuracy"
      ],
      "metadata": {
        "id": "EFw0qK8hh7kk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWjxSU3qiT_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}